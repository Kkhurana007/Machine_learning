{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6508b7f4",
   "metadata": {},
   "source": [
    "# learning outcomes \n",
    "\n",
    "    Diagnose dirty data\n",
    "    Side effects of dirty data\n",
    "    Cleaning data\n",
    "    \n",
    "    Steps involved-\n",
    "    Access > Explore and process > Extract > Report \n",
    "    \n",
    "    Types of data-\n",
    "    1) text\n",
    "    2) integers\n",
    "    3) decimals\n",
    "    4) binary\n",
    "    5) dates\n",
    "    6) categories\n",
    "    7) Numeric\n",
    "    \n",
    "    Pyhton data types\n",
    "    str, int, float, bool, datetime, category\n",
    "    \n",
    "    3 steps -\n",
    "    Stripping > Converting > Verifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c511dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing $ sign from the data\n",
    "\n",
    "\n",
    "# strip method\n",
    "sales['Revenue'] = sales ['Revenue'].str.strip('$')\n",
    "sales ['Revenue'] = sales ['Revenue'].astype('int')      #astype converts the argument to integers\n",
    "\n",
    "# check\n",
    "\n",
    "assert sales['Revenue'].dtype= 'int'       #nothing gets returned if the condition is met\n",
    "\n",
    "# example of this\n",
    "assert 1+1 == 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0e9f42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KHURAN~1\\AppData\\Local\\Temp/ipykernel_10980/2923017065.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# example of this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# example of this\n",
    "assert 1+1 == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6fce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of this\n",
    "assert 1+1 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57b3ec",
   "metadata": {},
   "source": [
    "# Numeric or Categorical data\n",
    "\n",
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')   #.as + type = helps to convert the data\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'                     # .d + type = check the kind of data\n",
    "\n",
    "# Print new summary statistics \n",
    "print(ride_sharing['user_type_cat'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4606af33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "206f35e2",
   "metadata": {},
   "source": [
    "# steps\n",
    "#1) took the data and stripped minutes from it. \n",
    "# 2) converted to the type integers\n",
    "# 3) checked if the data is in integers\n",
    "# 4) \n",
    "\n",
    "\n",
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
    "\n",
    "\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing['duration_time'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c29f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "598e0366",
   "metadata": {},
   "source": [
    "# Data range constraints\n",
    "\n",
    "        Revision-\n",
    "        how to plot a histogram\n",
    "        plt.hist(file_name['column'])\n",
    "        plt.title('Average rating of movies form 1-5')\n",
    "        \n",
    "# importing data and time function to clean the data\n",
    "    \n",
    "\n",
    "# Dealing with the out of range data\n",
    "\n",
    "\n",
    "# 2 ways of dropping Values- 1) filtering 2) .drop() - method\n",
    "\n",
    "    output= folder[folder['column'] <= 5]\n",
    "    \n",
    "    2) output = folder.drop(folder[folder['column'] > 5] .index, inplace = True)\n",
    "    \n",
    "# checking \n",
    "    assert folder['column'].max()<= 5\n",
    "    \n",
    "# Setting outer values to 5 even if it goes behond it\n",
    "\n",
    "    folder.loc[folder['column'] > 5, 'column'] =5 \n",
    "    \n",
    "    All the values got converted to 5 \n",
    "    \n",
    "    check \n",
    "    \n",
    "    assert folder['column'].max( <=5\n",
    "    # remember no output means it passed\n",
    "    \n",
    "# convert to date\n",
    "\n",
    "    file_name ['column']= pd.to_datetime(file_name['column']).dt.date\n",
    "    \n",
    "    \n",
    "    today_date = dt.date.today()\n",
    "       \n",
    "    # drop using filtering\n",
    "    file_name= file_name [file_name ['column'] < today_date]\n",
    "    \n",
    "    # using drop to drop values\n",
    "        file_name.drop(file_name[file_name['column'] > today_date]. index, inplace= True\n",
    "        \n",
    "        \n",
    "    Hardcore dates with upper limit\n",
    "    \n",
    "        file_name.loc[file_name['column'] > today_date, 'column'] = today_date\n",
    "        \n",
    "    # Assert is true\n",
    "    \n",
    "    assert file_name.column.max().date() <= today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3842cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5669e1d",
   "metadata": {},
   "source": [
    "# Convert ride_date to date\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "    Why we used .loc in place of drop here? \n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432557fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39b057de",
   "metadata": {},
   "source": [
    "# getting rid of  Duplicate values\n",
    "\n",
    "    we use .duplicated method()\n",
    "    \n",
    "    \n",
    "    The .duplicated() method\n",
    "    \n",
    "    subset : list of column names to check for duplication\n",
    "    keep : weather to keep first('first'), last('last') or all (False) duplicate values\n",
    "    \n",
    "    \n",
    "    \n",
    "    # checking for duplicates\n",
    "    \n",
    "    column_names = ['first_name', 'last_name', 'address']\n",
    "    \n",
    "        We choose to keep all the duplicates\n",
    "        \n",
    "    duplicates = file _name.duplicated (subset = column_names , keep = False) \n",
    "    \n",
    "    \n",
    "# output the duplicates by sort()    \n",
    "    We can find duplicate rows by .sort_values method\n",
    "    \n",
    "    file_name [duplicates].sort_values (by = 'first_name') \n",
    "\n",
    "\n",
    "# treating them by .groupby() and .agg() methods\n",
    "    file_name.drop_duplicates (inplace = True)\n",
    "    \n",
    "# will be grouping by column names and create duplicates\n",
    "\n",
    "    column_names = ['first_nameM, 'last_name', 'address']\n",
    "    \n",
    "    summaries = {'height' : 'max', 'weight' : 'mean'}\n",
    "    \n",
    "       a = a.groupby(by = column_names).agg(summaries).reset_index()          \n",
    "       \n",
    "       # we use .reset() to have numbered indices of the defined index\n",
    "       \n",
    "       \n",
    "       \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad3d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6293c701",
   "metadata": {},
   "source": [
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated (subset = 'ride_id', keep = False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values(by = 'ride_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1b234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b52746bd",
   "metadata": {},
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated (subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402c0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0899f0ce",
   "metadata": {},
   "source": [
    "# Text and categorical data problems\n",
    "    \n",
    "    3 types of data-\n",
    "    1) marriage status (unmarried, married) (numerical values- 0, 1) \n",
    "   \n",
    "    2) household income category (0-20k, 20-40k, ...) (numerical values - 0, 1, 2, ...) \n",
    "    \n",
    "    \n",
    "    3) loan status (default, paid, no loan) (0,1,2) \n",
    "    \n",
    "    \n",
    "## Kind of problems?\n",
    "    Data entry errors  (free text and drop down)\n",
    "    Parsing errors\n",
    "   \n",
    "## how to solve?\n",
    "    3 ways-\n",
    "    1) Dropping data   2) Remapping categories 3) Inferring categories\n",
    "    \n",
    "## concept? \n",
    "    1) Anti-joins  (mutually exclusive) A U B = what is in A, but not in B\n",
    "    2) Innter-joints  (mutually inclusive) A and B = common area waht is both in A and B\n",
    "    \n",
    "# let's try this in python\n",
    "    (I) inconsistent_categories = set (file_name['column_name']).difference(categories['column_name']\n",
    "    \n",
    "    \n",
    "    Step -2\n",
    "    \n",
    "    Get and print rows with inconsitent categories-\n",
    "    \n",
    "    (II) inconsistent_rows = file_name['column_name'].isin(inconsistent_categories) \n",
    "    \n",
    "    \n",
    "    (III) inconsistent_data = file_name [inconsistent_rows]\n",
    "    \n",
    "    (IV) consistent_data = file_name[ inconsistent_rows]\n",
    "    \n",
    "   \n",
    "    \n",
    "    This returns a series of boolean values for consistent and inconsistent ones\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90296973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a04d337f",
   "metadata": {},
   "source": [
    "# categorical variables (cleaning data in python)\n",
    "\n",
    "    1) value inconsistency (capitalization) \n",
    "    2) collapsing too many categories to a few\n",
    "    3) making sure data is of a right type\n",
    "    \n",
    "    \n",
    "    Methods-\n",
    "    \n",
    "    .value_counts() --for series\n",
    "        .groupby('column').count() - for DataFrame\n",
    "        \n",
    "     # capitalization\n",
    "     file_name['column_name']= file_name['column_name].str.upper()\n",
    "    \n",
    "    # remove spaces\n",
    "    file_name= file_name ['column_name'].str.strip()\n",
    "    \n",
    "    # getting output \n",
    "    file_name['column_name'].value_counts()\n",
    "    \n",
    "    \n",
    "# collapsing data into categories\n",
    "    # creating income_group from income column\n",
    "    \n",
    "    \n",
    "   ## using qcut () from pandas\n",
    "import pandas as pd\n",
    "       \n",
    "group_names = ['0-30K', '31K-59k', '60K-90K']\n",
    "demographics['income_group'] = pd.qcut(demographics['household_income'], q= 3, labels = group_names) \n",
    "       \n",
    "    # print the income_group column\n",
    "demographis[['income_group', 'household_income']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c11dcd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'demographics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KHURAN~1\\AppData\\Local\\Temp/ipykernel_5296/1140185524.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgroup_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'0-30K'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'31K-59k'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'60K-90K'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdemographics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'income_group'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdemographics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'household_income'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# print the income_group column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'demographics' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf393e44",
   "metadata": {},
   "source": [
    "##  using cut function with bins argument to do the same job\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "    ranges = [0, 20000, 5000, np.inf]\n",
    "    \n",
    "    demographics ['income group'] = pd.cut (demographics['household_income'], bins = ranges, labels = group_names) \n",
    "    \n",
    "    demographics [['inner_group', 'household_income']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ef13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe3fb3a7",
   "metadata": {},
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf141d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "669dc687",
   "metadata": {},
   "source": [
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47650c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5348505f",
   "metadata": {},
   "source": [
    "# cleaning text data\n",
    "\n",
    "    can of of type (names, phone numbers, emails, adresses, and more) \n",
    "    \n",
    "    problems-\n",
    "    1) inconsitent\n",
    "        2) length violations\n",
    "            3) typos\n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca3110",
   "metadata": {},
   "source": [
    "# fixing the phone number with the help of example\n",
    "\n",
    "# replace '+' with '00'\n",
    "\n",
    "phones['phonenumber']= phones['phonenumber'].str.replace('+', '00')   #takes in 2 values- strings being replaced\n",
    "                                                                                            replaced by what\n",
    "                                                                                            \n",
    "\n",
    "to get the output-\n",
    "phones\n",
    "\n",
    "\n",
    "\n",
    "# replacing numbers below 10 to nan\n",
    " \n",
    "digits = phones['phone number'].str.len()\n",
    "phones.loc[digits< 10, 'phone number'] = np.nan                   #replaced by numpy's nan object\n",
    "phones \n",
    "\n",
    "# checking \n",
    "assert digits.min() >= 10\n",
    "\n",
    "assert phone ['phone number'].str.contains ('+|-').any() == False\n",
    "\n",
    "\n",
    "\n",
    "# regular expression with wide ranging phone numbers in action\n",
    "\n",
    "phones['Phone number'] = phones['Phone number'].str.replace(r'\\D+', '') \n",
    "\n",
    "phones.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f5489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e8ad01a",
   "metadata": {},
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9cf5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5af2c810",
   "metadata": {},
   "source": [
    "# Advanced data problems  (uniformity) \n",
    "\n",
    "\n",
    "## quick revision for data visulaization\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "    Scatter\n",
    "plt.scatter(x = 'abc', y = 'def', data = temperatures)\n",
    "\n",
    "    Title, x-label, and y-label\n",
    "plt.title('Temperature in Celcuis March 2019 -NYC')\n",
    "\n",
    "plt.xlabel ('Dates')\n",
    "\n",
    "plt.ylabel('temperature in degree celcius')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## coming back, treating temperature data\n",
    "\n",
    "temp_fah = temperatures.loc[temperatures['Tempertaure'] > 40, 'Tempertaure']\n",
    "\n",
    "temp_cels = (temp_fah - 32) * (5/9) \n",
    "\n",
    "temperatures.loc[temperatures['Temperature'] > 40, 'Temperature'] = temp_cels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301de4a9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Treating the date data\n",
    "\n",
    "        convert to data time - but won't work\n",
    "        \n",
    "        birthdays['birhday'] = pd.to_datetime(birthdays['Birhtday']\n",
    "        \n",
    "        To make the above code work -\n",
    "        \n",
    "        \n",
    "        birthdays['birthday'] = pd.to_datetime(birthdays['Birthday'], infer_datetime_format = True, errors = 'coerce') \n",
    "        \n",
    "        \n",
    "        # remark, here- infer_datetime_format attempts of to format the date time\n",
    "        errors returns NaT for rows where conversion fails\n",
    "## another format\n",
    "\n",
    "        birthdays['birthday'] = birthdays['Birthday'].dt.strftime('%d-%m-%y') \n",
    "        birhtdays.head()\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e303d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030ee078",
   "metadata": {},
   "source": [
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print (banking['acct_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8394a",
   "metadata": {},
   "source": [
    "# Cross field validation\n",
    "1)summing of the columns to make sure that the total is equal to the required value\n",
    "\n",
    "\n",
    "## data integrity \n",
    "    Use of multiple fields to sanity check the integrity of the data!\n",
    "    \n",
    "    example \n",
    "    sum_classes = file_name [['column1', 'column2', 'column3']].sum(axis =1) \n",
    "    chekcing = sum_classes == file_name ['column_compared']\n",
    "    \n",
    "    inconsistent = file_name [~column_compared]\n",
    "    consistent = file_name[colum_compared]\n",
    "\n",
    "\n",
    "## using same case in date data\n",
    "    import pandas as pd\n",
    "    import datetime as dt\n",
    "    \n",
    "    convert to datetime and get today's date\n",
    "    \n",
    "    file_name ['column_compared']= pd.to_datetime(file_name['column_compared'])\n",
    "    \n",
    "    today = dt.date.today("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62869e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 3, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import datetime as dt\n",
    "\n",
    "today = dt.date.today()\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda283b1",
   "metadata": {},
   "source": [
    "# what to do in case of inconsistencies\n",
    "1) drop data\n",
    "2) Set to missing and impute\n",
    "3) Apply rules from domain knowledge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de5ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b0735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis= 1) == banking ['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking [~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdf09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09dd4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = banking['age'] == ages_manual\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "\n",
    "consistent_ages = banking [age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf7a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12e7bb66",
   "metadata": {},
   "source": [
    "# cleaning the missing data\n",
    "\n",
    "# finding the missiong values \n",
    "    we use .isna() method, which retuns True for all the missing values. \n",
    "    \n",
    "    Values include- NA, nan, 0, . \n",
    "    \n",
    "    \n",
    "    if we use .sum() method along with .isna(), it returns us the breakdown of missing values. \n",
    "    \n",
    "    for eg. - print (file_name.isna().sum())\n",
    "    \n",
    "# packages involved to find the missing values. \n",
    "    missingno\n",
    "    \n",
    "    import missingno as msno\n",
    "    \n",
    "## how to visualize the missing data?\n",
    "    msno.matrix(file_name)\n",
    "    \n",
    "    import missingo as msno\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # visaulaize missingness\n",
    "    msno.matrix (file_name)\n",
    "    plt.show()\n",
    "    \n",
    "## isolating missing and complete valaues of a variable aside\n",
    "\n",
    "        missing = file_name [file_name['column_name'].isna()]\n",
    "        complete = file_name [~file_name ['column_name'].isna()]\n",
    "        \n",
    "## thereafter, we use describe method to compare the results\n",
    "        complete.describe()\n",
    "        missing.describe()\n",
    "        \n",
    "# sorting the data according to us-\n",
    "    sorted_file_name = file_name.sort_values(by= 'column_name')\n",
    "    msno.matrix (sorted_file_name)\n",
    "    plt.show()\n",
    "    \n",
    "## missingness types-\n",
    "    1) missing completely at random (no systematic relationship) \n",
    "    2) missing at random (systematic realtionship between missing and other [observed] values)\n",
    "    3) missing Not at random (systematic relationship between missing and [unobserved] values)\n",
    "    \n",
    "# dealing with missing data (2 approaches) -\n",
    "## simple methods-\n",
    "    1) drop missing data\n",
    "    2) impute with statistical measures (mean, median, mode) \n",
    "\n",
    "## complex methods-\n",
    "    1) impute using algorithmic approach\n",
    "    2) impute using machine learning models\n",
    "    \n",
    "    \n",
    "# we<ll make use of simple methods in this!\n",
    "\n",
    "file_name_dropped = file_name.dropna(subset= ['column1']\n",
    "file_name_dropped.head()\n",
    "\n",
    "    1) replacing method- by mean\n",
    "    \n",
    "    file_name_mean = file_name['colomn1'].mean()\n",
    "    file_name_imputed = file_name.fillna({'column1' : column1_mean})\n",
    "    file_name_imputed.head()\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae65ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapsing incorrect categories of DataFrames with incorrect ones\n",
    "\n",
    "for state in categories ['state']:\n",
    "    # finding potential matches by setting limit to lenght of the survey DataFrame\n",
    "    matches= process.extract (state, survey['state'], limit = survey.shape[0])\n",
    "    \n",
    "    # iterating over each potential match, thus seperating the ones with match rating of higher than or equal to 80\n",
    "    for potential_match in matches:\n",
    "        # setting the similarity score to 80 or above\n",
    "        if potentail_match[1]  >= 80:\n",
    "            \n",
    "            # replacing it with the correct state with the loc method\n",
    "            survey.loc[survey['state']] == potential_match [0], 'state'] = state\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66054548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import process from fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurants['cuisine_type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit = len(unique_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f56fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the unique values of the cuisine_type column\n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8328a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b358139",
   "metadata": {},
   "source": [
    "    RECORD LINKING JOINS DATA WITH FUZZY DUPLICATE VALUES\n",
    "\n",
    "        Generally, we clean 2 or more data frames, generate pairs or potentially matching records, score these pairs according to string similarity and other similarity methods, and link them. \n",
    "        \n",
    "        All of these steps can be achieved with the record linking package. \n",
    "        \n",
    "        Generating pairs-\n",
    "        we start by importing the record linkage (called blocking) \n",
    "        \n",
    "        thereafter, we create an indexing object\n",
    "        \n",
    "        generally, we block the pairs on state\n",
    "        \n",
    "        \n",
    "        Comparing the DataFrames\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc584a",
   "metadata": {},
   "source": [
    "# Generating pairs in python\n",
    "\n",
    "    # Import record linkage first\n",
    "       [import recordlinkage]\n",
    "       \n",
    "     # create index object \n",
    "     indexer = recordlinkage.Index()\n",
    "     \n",
    "     \n",
    "     # generate pairs blocked on state\n",
    "     indexer.block('state') \n",
    "     pairs = indexer.index(census_A, census_B)\n",
    "     \n",
    "     \n",
    "     # generate the pairs\n",
    "       pairs = indexer.index (census_A, census_B) \n",
    "   \n",
    "     print (pairs) \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e92313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "561321a3",
   "metadata": {},
   "source": [
    "# comparing the data frames\n",
    "\n",
    "    generate the pairs as before-\n",
    "       pairs = indexer.index (census_A, census_B)\n",
    "     \n",
    "     create a compare object- \n",
    "     compare_columns = recordlinkage.Comapre()\n",
    "     \n",
    "     \n",
    "     # find the exact matches of pairs of date_of_birth and state\n",
    "     \n",
    "    compare_columns.extract('date_of_birth', 'date_of_birth', label = 'date_of_birth')\n",
    "    compare_columns.extract('state', 'state', label = 'state')\n",
    "    \n",
    "    # now using the string similarity\n",
    "    compare_columns.string('surname', 'surname', threshold = 0.85, label = 'surname')\n",
    "       compare_columns.string('address_1', 'address_1', threshold = 0.85, label = 'address_1') \n",
    "       \n",
    "       \n",
    "       # find matches between possible pairs and datasets in question\n",
    "       \n",
    "       potential_matches = compare_columns.compute(pairs, census_A, census_B)  \n",
    "       \n",
    "       \n",
    "       #output \n",
    "       print (potential_matches)\n",
    "       \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7cf27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd989558",
   "metadata": {},
   "source": [
    "# finding the only pairs we want\n",
    "\n",
    "potential_matches  [potential_matches.sum(axix =1) => 2]\n",
    "\n",
    "# useful article for record_linkage\n",
    "https://medium.com/data-science-business/record-linkage-merging-disparate-datasets-8aa02a2e4535 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a2264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49edff6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_7828/4053469300.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\KHURAN~1\\AppData\\Local\\Temp/ipykernel_7828/4053469300.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    @book{howard2020deep,\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@book{howard2020deep,\n",
    "title={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},\n",
    "author={Howard, J. and Gugger, S.},\n",
    "isbn={9781492045526},\n",
    "url={https://books.google.no/books?id=xd6LxgEACAAJ},\n",
    "year={2020},\n",
    "publisher={O'Reilly Media, Incorporated}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279c7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
